{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejazfahil/Hardware-and-software-project/blob/main/FSF_StreamLit_HW_SW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLjCPexUP_O6",
        "outputId": "11f534e9-92db-4145-e2db-8eb253171a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.14.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install pmdarima\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VxsS70ZSvUB7",
        "outputId": "53ff6972-c157-4e9c-a3ca-e63c8fcbdb2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\\n!unzip ngrok-stable-linux-amd64.zip\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pTyEN3xEvZEH"
      },
      "outputs": [],
      "source": [
        "# get_ipython().system_raw('./ngrok http 8501 &')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ILeCdwyZveuo",
        "outputId": "2c023187-a57c-4e9f-fe55-cdd81864d8e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!curl -s http://localhost:4040/api/tunnels | python3 -c     \"import sys, json; print(json.load(sys.stdin)[\\'tunnels\\'][0][\\'public_url\\'])\"\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gdyAEJetQTNl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "72a8c160-188b-4adc-996d-727261e8bf30"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret KAGGLE_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d1114a568e89>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KAGGLE_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KAGGLE_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KAGGLE_USERNAME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KAGGLE_USERNAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret KAGGLE_KEY does not exist."
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj7Ej9EfQkl5",
        "outputId": "ec3313af-d8f1-4607-fb00-067eba39a41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/anlgrbz/student-demographics-online-education-dataoulad\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading student-demographics-online-education-dataoulad.zip to /content\n",
            "100% 42.0M/42.2M [00:00<00:00, 46.3MB/s]\n",
            "100% 42.2M/42.2M [00:00<00:00, 50.1MB/s]\n",
            "Archive:  student-demographics-online-education-dataoulad.zip\n",
            "  inflating: assessments.csv         \n",
            "  inflating: courses.csv             \n",
            "  inflating: studentAssessment.csv   \n",
            "  inflating: studentInfo.csv         \n",
            "  inflating: studentRegistration.csv  \n",
            "  inflating: studentVle.csv          \n",
            "  inflating: vle.csv                 \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d anlgrbz/student-demographics-online-education-dataoulad\n",
        "\n",
        "! unzip \"student-demographics-online-education-dataoulad.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1lnhmPQRGCe",
        "outputId": "c1b63eb5-4de7-4190-d447-7fb8981b53fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import plot_tree\n",
        "from sklearn.metrics import mean_squared_error , mean_absolute_error, r2_score\n",
        "from pmdarima.arima.utils import nsdiffs\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title='Open University Learning Analytics Dataset (OULAD) Analysis',\n",
        "    page_icon='ðŸ“ˆ'\n",
        ")\n",
        "\n",
        "# -------------------------------------Functions--------------------------------------------\n",
        "# -------------------------------------Functions--------------------------------------------\n",
        "# -------------------------------------Functions--------------------------------------------\n",
        "# -------------------------------------Functions--------------------------------------------\n",
        "\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    assesment = pd.read_csv('/content/assessments.csv')\n",
        "    course = pd.read_csv('/content/courses.csv')\n",
        "    as_stu = pd.read_csv('/content/studentAssessment.csv')\n",
        "    info_stu = pd.read_csv('/content/studentInfo.csv')\n",
        "    reg_stu = pd.read_csv('/content/studentRegistration.csv')\n",
        "    vle_stu = pd.read_csv('/content/studentVle.csv')\n",
        "    vle = pd.read_csv('/content/vle.csv')\n",
        "    return assesment, course, as_stu, info_stu, reg_stu, vle_stu, vle\n",
        "\n",
        "assesment, course, as_stu, info_stu, reg_stu, vle_stu, vle = load_data()\n",
        "\n",
        "def missingValueAssessment(data):\n",
        "    st.write(\"## Data Information\")\n",
        "    buffer = io.StringIO()\n",
        "    data.info(buf=buffer)\n",
        "    s = buffer.getvalue()\n",
        "    st.text(s)\n",
        "\n",
        "    st.write(\"Dataframe Shape:: \", data.shape)\n",
        "\n",
        "    st.write(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "    missing_values = data.isnull().sum()\n",
        "    st.write(\"### Missing Values\")\n",
        "    st.write(missing_values)\n",
        "\n",
        "    st.write(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "    missing_values_percentage = (missing_values / len(data)) * 100\n",
        "\n",
        "    # Create a DataFrame to display missing values\n",
        "    missing_values_df = pd.DataFrame({\n",
        "        'Column': missing_values.index,\n",
        "        'Missing Values': missing_values.values,\n",
        "        'Percentage': missing_values_percentage\n",
        "    })\n",
        "\n",
        "    # Display the missing values\n",
        "    st.write(\"### Missing Values DataFrame\")\n",
        "    st.write(missing_values_df)\n",
        "\n",
        "    st.write(\"----------------------------------------------------------------------------------\")\n",
        "\n",
        "    # Visualize the missing values\n",
        "    st.write(\"### Missing Values Visualization\")\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Percentage', y='Column', data=missing_values_df.sort_values(by='Percentage', ascending=False))\n",
        "    plt.title('Percentage of Missing Values by Column')\n",
        "    plt.xlabel('Percentage of Missing Values')\n",
        "    plt.ylabel('Columns')\n",
        "\n",
        "    st.pyplot(plt)\n",
        "\n",
        "\n",
        "def plotActivityCounts(data):\n",
        "    activity_counts = data['activity_type'].value_counts().reset_index()\n",
        "    activity_counts.columns = ['activity_type', 'count']\n",
        "\n",
        "    # Sort the activity types by count in descending order\n",
        "    activity_counts = activity_counts.sort_values(by='count', ascending=False)\n",
        "\n",
        "    # Plot the bar chart\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    bar_plot = sns.barplot(x='activity_type', y='count', data=activity_counts, palette='husl', hue='activity_type', dodge=False)\n",
        "\n",
        "    # Set the title and labels\n",
        "    plt.title('Count of Each Activity Type', fontsize=16)\n",
        "    plt.xlabel('Activity Type', fontsize=14)\n",
        "    plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "    # Rotate the x labels for better readability\n",
        "    bar_plot.set_xticklabels(bar_plot.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "\n",
        "    st.pyplot(plt.gcf())  # Use st.pyplot to display the plot in Streamlit\n",
        "\n",
        "def student_vle_eda(data):\n",
        "  daily_interactions = data.groupby('date')['sum_click'].sum().reset_index()\n",
        "\n",
        "  # Set date as index\n",
        "  daily_interactions.set_index('date', inplace=True)\n",
        "\n",
        "  # Display the first few rows\n",
        "  print(daily_interactions.head())\n",
        "\n",
        "  # Plot the time series data\n",
        "  plt.figure(figsize=(14, 7))\n",
        "  plt.plot(daily_interactions['sum_click'], label='Daily Interactions')\n",
        "  plt.title('Daily Student Interactions with VLE')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Sum of Clicks')\n",
        "  plt.legend()\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "def stu_reg_eda(data):\n",
        "\n",
        "  st.subheader('Registration Count of Students With Time')\n",
        "  #Aggregate interactions per day\n",
        "  daily_interactions = data.groupby('date_registration')['id_student'].sum().reset_index()\n",
        "\n",
        "  # Set date as index\n",
        "  daily_interactions.set_index('date_registration', inplace=True)\n",
        "\n",
        "  # Display the first few rows\n",
        "  print(daily_interactions.head())\n",
        "\n",
        "  # Plot the time series data\n",
        "  plt.figure(figsize=(14, 7))\n",
        "  plt.plot(daily_interactions['id_student'], label='Registration Count')\n",
        "  plt.title('Count of Student Registration with time')\n",
        "  plt.xlabel('Date of Registration')\n",
        "  plt.ylabel('Count of Students')\n",
        "  plt.legend()\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "\n",
        "  st.subheader('Registration Count Per Module of Students With Time')\n",
        "\n",
        "  # Aggregate the registration counts per module over time\n",
        "  registration_trends = data.groupby(['date_registration', 'code_module']).size().reset_index(name='count')\n",
        "\n",
        "  # Get the list of unique modules\n",
        "  modules = registration_trends['code_module'].unique()\n",
        "\n",
        "  # Set the figure size for each plot\n",
        "  plt.figure(figsize=(14, 7))\n",
        "\n",
        "  # Loop through each module and create a separate plot\n",
        "  for module in modules:\n",
        "      plt.figure(figsize=(14, 7))\n",
        "      module_data = registration_trends[registration_trends['code_module'] == module]\n",
        "      sns.lineplot(x='date_registration', y='count', data=module_data, marker='o')\n",
        "\n",
        "      # Set the title and labels\n",
        "      plt.title(f'Student Registration Trends for Module {module}', fontsize=16)\n",
        "      plt.xlabel('Date of Registration', fontsize=14)\n",
        "      plt.ylabel('Count of Registrations', fontsize=14)\n",
        "\n",
        "      # Rotate the x-axis labels for better readability\n",
        "      plt.xticks(rotation=45)\n",
        "\n",
        "      # Display the plot\n",
        "      st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "def info_stu_eda(info_stu):\n",
        "\n",
        "  st.subheader('Head of the data frame')\n",
        "\n",
        "  st.write(info_stu.head(10))\n",
        "\n",
        "  st.subheader('Count Of Exam Results')\n",
        "\n",
        "  activity_counts = info_stu['final_result'].value_counts().reset_index()\n",
        "  activity_counts.columns = ['final_result', 'count']\n",
        "\n",
        "  # Sort the final_result by count in descending order\n",
        "  activity_counts = activity_counts.sort_values(by='count', ascending=False)\n",
        "\n",
        "  # Plot the bar chart\n",
        "  plt.figure(figsize=(14, 7))\n",
        "  sns.set_style(\"whitegrid\")\n",
        "  bar_plot = sns.barplot(x='final_result', y='count', data=activity_counts, palette='Set2', hue = \"final_result\", legend=False)\n",
        "\n",
        "  # Set the title and labels\n",
        "  plt.title('Count of Exam Result', fontsize=16)\n",
        "  plt.xlabel('Result Status', fontsize=14)\n",
        "  plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "  # Rotate the x labels for better readability\n",
        "  bar_plot.set_xticklabels(bar_plot.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "\n",
        "  # Display the plot\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "  #--------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  st.subheader('Final Results by Gender')\n",
        "\n",
        "  # Aggregate data by final result and gender\n",
        "  final_result_gender_counts = info_stu.groupby(['final_result', 'gender']).size().reset_index(name='count')\n",
        "\n",
        "  # Sort final results by count in descending order\n",
        "  sorted_results = final_result_gender_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "\n",
        "  # Plot the bar chart\n",
        "  plt.figure(figsize=(14, 7))\n",
        "  sns.barplot(x='final_result', y='count', hue='gender', data=final_result_gender_counts, palette='Set2', order=sorted_results)\n",
        "\n",
        "  # Set the title and labels\n",
        "  plt.title('Final Results by Gender (Sorted)', fontsize=16)\n",
        "  plt.xlabel('Final Result', fontsize=14)\n",
        "  plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "  # Display the plot\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "  #--------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  st.subheader('Final Results by Region')\n",
        "\n",
        "  # Aggregate data by final result and region\n",
        "  final_result_region_counts = info_stu.groupby(['final_result', 'region']).size().reset_index(name='count')\n",
        "\n",
        "  # Sort final results by count in descending order\n",
        "  sorted_results = final_result_region_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "\n",
        "  # Pivot the data to have regions as columns\n",
        "  pivot_data = final_result_region_counts.pivot(index='final_result', columns='region', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "  # Plot the stacked bar chart\n",
        "  pivot_data.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='Set2')\n",
        "\n",
        "  # Set the title and labels\n",
        "  plt.title('Final Results by Region (Stacked)', fontsize=16)\n",
        "  plt.xlabel('Final Result', fontsize=14)\n",
        "  plt.ylabel('Count', fontsize=14)\n",
        "  plt.xticks(rotation=45)\n",
        "\n",
        "  # Display the plot\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "  #--------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  st.subheader('Final Results by Highest Education')\n",
        "\n",
        "  # Aggregate data by final result and highest education\n",
        "  final_result_education_counts = info_stu.groupby(['final_result', 'highest_education']).size().reset_index(name='count')\n",
        "\n",
        "  # Sort final results by count in descending order\n",
        "  sorted_results = final_result_education_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "\n",
        "  # Pivot the data to have highest education as columns\n",
        "  pivot_data = final_result_education_counts.pivot(index='final_result', columns='highest_education', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "  # Plot the stacked bar chart\n",
        "  pivot_data.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='Set2')\n",
        "\n",
        "  # Set the title and labels\n",
        "  plt.title('Final Results by Highest Education (Stacked)', fontsize=16)\n",
        "  plt.xlabel('Final Result', fontsize=14)\n",
        "  plt.ylabel('Count', fontsize=14)\n",
        "  plt.xticks(rotation=45)\n",
        "\n",
        "  # Display the plot\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "  #--------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  st.subheader('Final Results by Module')\n",
        "\n",
        "  # Aggregate data by final result and highest education\n",
        "  final_result_education_counts = info_stu.groupby(['final_result', 'code_module']).size().reset_index(name='count')\n",
        "\n",
        "  # Sort final results by count in descending order\n",
        "  sorted_results = final_result_education_counts.groupby('final_result').sum().sort_values(by='count', ascending=False).index\n",
        "\n",
        "  # Pivot the data to have highest education as columns\n",
        "  pivot_data = final_result_education_counts.pivot(index='final_result', columns='code_module', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "  # Plot the stacked bar chart\n",
        "  pivot_data.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='Set2')\n",
        "\n",
        "  # Set the title and labels\n",
        "  plt.title('Final Results by Module (Stacked)', fontsize=16)\n",
        "  plt.xlabel('Final Result', fontsize=14)\n",
        "  plt.ylabel('Count', fontsize=14)\n",
        "  plt.xticks(rotation=45)\n",
        "\n",
        "  # Display the plot\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "  #--------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  st.subheader('Final Results by Module Grouped by Module')\n",
        "\n",
        "  mod = info_stu['code_module'].unique()\n",
        "\n",
        "  for mname in mod:\n",
        "    data = info_stu[info_stu['code_module'] == mname]\n",
        "\n",
        "    final_result_education_counts = data.groupby(['final_result', 'highest_education']).size().reset_index(name='count')\n",
        "\n",
        "    # Sort final results by count in descending order\n",
        "    sorted_results = final_result_education_counts.groupby('final_result').sum().index\n",
        "\n",
        "    # Pivot the data to have highest education as columns\n",
        "    pivot_data = final_result_education_counts.pivot(index='final_result', columns='highest_education', values='count').reindex(sorted_results).fillna(0)\n",
        "\n",
        "    # Plot the stacked bar chart\n",
        "    pivot_data.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='Set2')\n",
        "\n",
        "    # Set the title and labels\n",
        "    plt.title(f'Final Results by Highest Education (Stacked) - Module {mname}', fontsize=16)\n",
        "    plt.xlabel('Final Result', fontsize=14)\n",
        "    plt.ylabel('Count', fontsize=14)\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Display the plot\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "  #--------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "  st.subheader('Correlation Matrix')\n",
        "\n",
        "  student_info = info_stu.copy()\n",
        "\n",
        "  # Preprocess the data\n",
        "  # Convert categorical columns to numerical\n",
        "  student_info['gender'] = student_info['gender'].map({'M': 0, 'F': 1})\n",
        "  student_info['region'] = student_info['region'].astype('category').cat.codes\n",
        "  student_info['highest_education'] = student_info['highest_education'].astype('category').cat.codes\n",
        "  student_info['imd_band'] = student_info['imd_band'].astype('category').cat.codes\n",
        "  student_info['age_band'] = student_info['age_band'].astype('category').cat.codes\n",
        "  student_info['disability'] = student_info['disability'].map({'N': 0, 'Y': 1})\n",
        "  student_info['code_module'] = student_info['code_module'].astype('category').cat.codes\n",
        "  student_info['code_presentation'] = student_info['code_presentation'].astype('category').cat.codes\n",
        "\n",
        "  # Map final result to numerical values\n",
        "  final_result_map = {\n",
        "      'Distinction': 4,\n",
        "      'Pass': 3,\n",
        "      'Fail': 2,\n",
        "      'Withdrawn': 1\n",
        "  }\n",
        "  student_info['final_result'] = student_info['final_result'].map(final_result_map)\n",
        "\n",
        "  # Calculate the correlation matrix\n",
        "  correlation_matrix = student_info.corr()\n",
        "\n",
        "  # Plot the correlation matrix\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  sns.heatmap(correlation_matrix, annot=True, cmap='Set2', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "  # Set the title\n",
        "  plt.title('Correlation Matrix with Final Result as Target', fontsize=16)\n",
        "\n",
        "  # Display the plot\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "def as_stu_eda(as_stu):\n",
        "  st.subheader('Head of Student Information Data')\n",
        "\n",
        "  st.write(as_stu.head(10))\n",
        "\n",
        "  st.subheader('Score Trend Over Time')\n",
        "  Average_score = as_stu.groupby('date_submitted')['score'].mean().reset_index()\n",
        "  Average_score['Score_Category'] = pd.cut(Average_score['score'], bins=[-float('inf'), 40, float('inf')], labels=['Fail (< 40)', 'Pass (>= 40)'])\n",
        "\n",
        "  # Create a simple trend plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.lineplot(data=Average_score, x='date_submitted', y='score', marker='o', style='Score_Category', hue='Score_Category', palette='Set2', dashes = False)\n",
        "\n",
        "  # Add title and labels\n",
        "  plt.title('Score Trend Over Time')\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Score')\n",
        "\n",
        "  # Rotate the x labels for better readability\n",
        "  plt.xticks(rotation=45)\n",
        "\n",
        "  # Add grid\n",
        "  plt.grid(True)\n",
        "\n",
        "  # Show plot\n",
        "  plt.tight_layout()\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "@st.cache_data\n",
        "def data_merge(as_stu, info_stu):\n",
        "\n",
        "  info_stu_cleaned = info_stu.copy()\n",
        "  info_stu_cleaned['imd_band'] = info_stu_cleaned['imd_band'].fillna('unknown', inplace=True)\n",
        "\n",
        "\n",
        "  as_stu_cleaned = as_stu.copy()\n",
        "  student_means = as_stu_cleaned.groupby('id_student')['score'].transform('mean')\n",
        "  as_stu_cleaned = as_stu_cleaned.dropna()\n",
        "  #st.write(as_stu_cleaned.isna().sum())\n",
        "  #as_stu_cleaned['score'] = as_stu_cleaned['score'].fillna(student_means[student_means['id_student'] == as_stu_cleaned['id_student']], inplace=True)\n",
        "  as_stu_cleaned['date_submitted'] = as_stu_cleaned['date_submitted'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "\n",
        "  data = pd.merge(as_stu_cleaned, info_stu_cleaned, on=['id_student'], how='left')\n",
        "  data['score'] = data['score'].fillna(0)\n",
        "\n",
        "  # Convert categorical columns to numerical\n",
        "  data['gender'] = data['gender'].map({'M': 0, 'F': 1})\n",
        "  data['region'] = data['region'].astype('category').cat.codes\n",
        "  data['highest_education'] = data['highest_education'].astype('category').cat.codes\n",
        "  data['imd_band'] = data['imd_band'].astype('category').cat.codes\n",
        "  data['age_band'] = data['age_band'].astype('category').cat.codes\n",
        "  data['disability'] = data['disability'].map({'N': 0, 'Y': 1})\n",
        "  data['code_module'] = data['code_module'].astype('category').cat.codes\n",
        "  data['code_presentation'] = data['code_presentation'].astype('category').cat.codes\n",
        "  final_result_map = {\n",
        "      'Distinction': 4,\n",
        "      'Pass': 3,\n",
        "      'Fail': 2,\n",
        "      'Withdrawn': 1\n",
        "  }\n",
        "  data['final_result'] = data['final_result'].map(final_result_map)\n",
        "\n",
        "  return data\n",
        "\n",
        "def decision_tree(student_info_assessments):\n",
        "\n",
        "    st.subheader('Decision Tree Regressor')\n",
        "\n",
        "    st.write(\"\"\"\n",
        "    my_dt = DecisionTreeRegressor(\n",
        "        max_depth=10,                 Limits depth of the tree\n",
        "        min_samples_split=15,        Requires at least 15 samples to consider a split\n",
        "        min_samples_leaf=5,         Requires at least 5 samples per leaf\n",
        "        max_leaf_nodes=200,          Maximum number of leaf nodes\n",
        "    )\n",
        "    my_dt.fit(X_train, y_train)  # Fit to training data\n",
        "    \"\"\")\n",
        "\n",
        "    # Drop rows with any missing values in the features and target\n",
        "    data = student_info_assessments.dropna(subset=['score'])\n",
        "    X = data.drop(columns=['id_student', 'score']).select_dtypes(include=[np.number])\n",
        "    y = data['score']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize the DecisionTreeRegressor with specific parameters to regularize the tree\n",
        "    my_dt = DecisionTreeRegressor(\n",
        "        max_depth=10,                # Limits depth of the tree\n",
        "        min_samples_split=15,       # Requires at least 20 samples to consider a split\n",
        "        min_samples_leaf=5,        # Requires at least 10 samples per leaf\n",
        "        max_leaf_nodes=200,         # Maximum number of leaf nodes\n",
        "    )\n",
        "    my_dt.fit(X_train, y_train)  # Fit to training data\n",
        "\n",
        "    # Predict and calculate R2 score\n",
        "    y_pred = my_dt.predict(X_test)\n",
        "\n",
        "    st.write(\"MSE: \", mean_squared_error(y_test, y_pred))\n",
        "    st.write(\"MAE: \", mean_absolute_error(y_test, y_pred))\n",
        "    st.write(\"R2 Score: \", r2_score(y_test, y_pred))\n",
        "\n",
        "    # Plot the decision tree\n",
        "    plt.figure(figsize=(20, 10))  # Set the size of the plot according to your preference\n",
        "    plot_tree(my_dt, feature_names=X.columns, filled=True)\n",
        "    plt.title('Decision Tree Visualization')\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    # Plot actual vs predicted values\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(y_test.values, label='Actual Values', marker='o')\n",
        "    plt.plot(y_pred, label='Predicted Values', marker='x')\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    st.subheader('Decision Tree Regressor with Pruning')\n",
        "\n",
        "    # Get the cost complexity pruning path\n",
        "    path = my_dt.cost_complexity_pruning_path(X_train, y_train)\n",
        "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "    st.write('Total Impurity vs effective alpha for training set')\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
        "    plt.xlabel(\"effective alpha\")\n",
        "    plt.ylabel(\"total impurity of leaves\")\n",
        "    plt.title(\"Total Impurity vs effective alpha for training set\")\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    alpha_selected = ccp_alphas[2]\n",
        "\n",
        "    st.write('Optimal alpha selected is:: ', alpha_selected)\n",
        "\n",
        "    # Re-train the tree with the selected alpha\n",
        "    my_dt_pruned = DecisionTreeRegressor(\n",
        "        random_state=44,\n",
        "        max_depth=10,\n",
        "        min_samples_split=15,\n",
        "        min_samples_leaf=5,\n",
        "        max_leaf_nodes=20,\n",
        "        ccp_alpha=alpha_selected\n",
        "    )\n",
        "    my_dt_pruned.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the pruned tree\n",
        "    y_pred = my_dt_pruned.predict(X_test)\n",
        "    st.write(\"MSE after pruning:\", mean_squared_error(y_test, y_pred))\n",
        "    st.write(\"MAE after pruning:\", mean_absolute_error(y_test, y_pred))\n",
        "    st.write(\"R2 Score: \", r2_score(list(y_test), list(my_dt_pruned.predict(X_test))))\n",
        "\n",
        "    # Plot the decision tree\n",
        "    plt.figure(figsize=(20,10))  # Set the size of the plot according to your preference\n",
        "    plot_tree(my_dt_pruned, feature_names=X.columns, filled=True)\n",
        "    plt.title('Pruned Decision Tree Visualization')\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "    # Plot actual vs predicted values\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(y_test.values, label='Actual Values', marker='o')\n",
        "    plt.plot(y_pred, label='Predicted Values', marker='x')\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "def linear_reg(student_info_assessments):\n",
        "\n",
        "  st.subheader('Linear Regression')\n",
        "\n",
        "  st.write(\"\"\"\n",
        "  my_lm = LinearRegression()\n",
        "  \"\"\")\n",
        "\n",
        "\n",
        "  data = student_info_assessments.dropna(subset=['score'])\n",
        "  X = data.drop(columns=['id_student', 'score']).select_dtypes(include=[np.number])\n",
        "  y = data['score']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  my_lm = LinearRegression()\n",
        "  my_lm.fit(X = X_train, y = y_train)\n",
        "\n",
        "  y_pred = my_lm.predict(X_test)\n",
        "  st.write(\"MSE \", mean_squared_error(y_test, y_pred))\n",
        "  st.write(\"MAE \", mean_absolute_error(y_test, y_pred))\n",
        "  st.write(\"R2_score \", r2_score(y_test, y_pred))\n",
        "\n",
        "  results = pd.DataFrame({'Actual': y_test[1:1000], 'Predicted': y_pred[1:1000]})\n",
        "\n",
        "  # Plot the results\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.scatterplot(x='Actual', y='Predicted', data=results, alpha=0.5, color='green')\n",
        "  sns.lineplot(x='Actual', y='Actual', data=results, color='red')  # Diagonal line\n",
        "\n",
        "  # Add titles and labels\n",
        "  plt.title('Actual vs Predicted Scores')\n",
        "  plt.xlabel('Actual Scores')\n",
        "  plt.ylabel('Predicted Scores')\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "def xgboost_mdl(student_info_assessments):\n",
        "\n",
        "  st.subheader('XGBoost')\n",
        "\n",
        "  st.write(\"\"\"\n",
        "    my_xgb = XGBRegressor()\n",
        "  \"\"\")\n",
        "\n",
        "\n",
        "  data = student_info_assessments.dropna(subset=['score'])\n",
        "  X = data.drop(columns=['id_student', 'score']).select_dtypes(include=[np.number])\n",
        "  y = data['score']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  my_xgb = XGBRegressor()\n",
        "  my_xgb.fit(X_train, y_train)\n",
        "\n",
        "  xgb_fcst = my_xgb.predict(X_test)\n",
        "  st.write(\"MSE \", mean_squared_error(y_test, xgb_fcst))\n",
        "  st.write(\"MAE \", mean_absolute_error(y_test, xgb_fcst))\n",
        "  st.write(r2_score(y_test, xgb_fcst))\n",
        "\n",
        "  plot_xgb_fcst = xgb_fcst[1:100]\n",
        "  plot_y_test = y_test[1:100]\n",
        "\n",
        "  #Create a graph to compare the XGBoost forecast to the actuals\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.plot(list(plot_xgb_fcst))\n",
        "  plt.plot(list(plot_y_test))\n",
        "  plt.legend(['actual', 'xgb forecast'])\n",
        "  plt.ylabel('Score')\n",
        "  plt.xlabel('Steps in test data')\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "def arima(df):\n",
        "\n",
        "  st.subheader('ARIMA')\n",
        "\n",
        "  st.write(\"\"\"\n",
        "  model = auto_arima(train_target,\n",
        "                      start_p=1, start_q=1,\n",
        "                      test='adf',       # Use ADF test to find optimal 'd'\n",
        "                      max_p=3, max_q=3, # Maximum p and q\n",
        "                      m=1,              # Frequency of series\n",
        "                      d=0,           # Let model determine 'd'\n",
        "                      seasonal=False,   # No seasonality\n",
        "                      start_P=0,\n",
        "                      trace=True,       # Print status\n",
        "                      error_action='ignore',\n",
        "                      suppress_warnings=True,\n",
        "                      stepwise=True)    # Apply stepwise algorithm\n",
        "  \"\"\")\n",
        "\n",
        "\n",
        "  data = df[['date_submitted','score']].copy()\n",
        "\n",
        "  # Set date_submitted as index]\n",
        "  data = data.set_index('date_submitted')\n",
        "\n",
        "  st.subheader('Testing for Stationarity')\n",
        "\n",
        "  for col in list(data):\n",
        "    d = nsdiffs(data[col],\n",
        "            m=10,\n",
        "            max_D=12,\n",
        "            test='ch')\n",
        "\n",
        "    st.write('Columns:: ', col, ' || d:: ', d)\n",
        "\n",
        "  plot_acf(data['score'], lags=40)\n",
        "  plot_pacf(data['score'], lags=40)\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "  data_daily = data.resample('W').mean()\n",
        "  data_daily = data_daily.sort_index()\n",
        "  data_daily = data_daily.interpolate(method='time')\n",
        "\n",
        "  plt.plot(data_daily['score'])\n",
        "  plt.title(f'Trend for score ')\n",
        "  plt.xlabel('Date Submitted')\n",
        "  plt.ylabel('Score')\n",
        "  plt.show()\n",
        "\n",
        "  adf_test = adfuller(data_daily['score'])\n",
        "  # Output the results\n",
        "  st.write('ADF Statistic: %f' % adf_test[0])\n",
        "  st.write('p-value: %f' % adf_test[1])\n",
        "  st.write('Since p-value < 0.05, stationarity doesnt exist (d = 0)')\n",
        "\n",
        "  train_size = int(len(data_daily) * 0.8)\n",
        "  train_data, test_data = data_daily.iloc[:train_size], data_daily.iloc[train_size:]\n",
        "\n",
        "  # Target variable\n",
        "  train_target = train_data['score']\n",
        "  test_target = test_data['score']\n",
        "\n",
        "  #Running Auto ARIMA\n",
        "  model = auto_arima(train_target,\n",
        "                        start_p=1, start_q=1,\n",
        "                        test='adf',       # Use ADF test to find optimal 'd'\n",
        "                        max_p=1, max_q=1, # Maximum p and q\n",
        "                        m=1,              # Frequency of series\n",
        "                        d=0,           # Let model determine 'd'\n",
        "                        seasonal=False,   # No seasonality\n",
        "                        start_P=0,\n",
        "                        trace=True,       # Print status\n",
        "                        error_action='ignore',\n",
        "                        suppress_warnings=True,\n",
        "                        stepwise=True)    # Apply stepwise algorithm\n",
        "\n",
        "  st.subheader('Arima Model Summary')\n",
        "  st.write(model.summary())\n",
        "\n",
        "  st.subheader('Forecasting')\n",
        "\n",
        "  #forecasting\n",
        "  n_periods = len(test_target)\n",
        "  forecast, conf_int = model.predict(n_periods=n_periods, return_conf_int=True)\n",
        "\n",
        "  # Convert predictions to a DataFrame\n",
        "  forecast_df = pd.DataFrame(forecast, index=test_target.index, columns=['Forecast'])\n",
        "  conf_int_df = pd.DataFrame(conf_int, index=test_target.index, columns=['Lower CI', 'Upper CI'])\n",
        "\n",
        "  mae = mean_absolute_error(test_target, forecast)\n",
        "  rmse = np.sqrt(mean_squared_error(test_target, forecast))\n",
        "  st.write(f'MAE: {mae}')\n",
        "  st.write(f'RMSE: {rmse}')\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.plot(train_target, label='Train')\n",
        "  plt.plot(test_target, label='Test')\n",
        "  plt.plot(forecast_df, label='Forecast')\n",
        "  plt.fill_between(conf_int_df.index,\n",
        "                  conf_int_df['Lower CI'],\n",
        "                  conf_int_df['Upper CI'],\n",
        "                  color='k', alpha=.15)\n",
        "  plt.legend()\n",
        "  st.pyplot(plt.gcf())\n",
        "\n",
        "\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "# -------------------------------------Main Code--------------------------------------------\n",
        "\n",
        "\n",
        "# Title\n",
        "st.title('Open University Learning Analytics Dataset (OULAD) Analysis')\n",
        "\n",
        "# Subheading\n",
        "st.markdown(\"### Dataset Visualization and Analysis\")\n",
        "\n",
        "# Sidebar for navigation\n",
        "st.sidebar.header(\"Contents\")\n",
        "sections = [\n",
        "    \"Introduction\", \"Missing Value Analysis\",\"Exploratory Data Analysis (EDA)\", \"Machine Learning (ML)\"\n",
        "]\n",
        "\n",
        "start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "choice = st.sidebar.radio(\"Select a section:\", sections)\n",
        "\n",
        "# Introduction to Streamlit\n",
        "if choice == \"Introduction\":\n",
        "\n",
        "    st.header(\"Introduction to Dataset\")\n",
        "    st.markdown(\"\"\"\n",
        "    This dataset belongs to Open University Online Learning Platform (Also called as \"Virtual Learning Environment(VLE)\") that off-campus students use for accessing the course content, forum discussions, sending assessments and checking out assignment marks etc. It consists of 7 selected courses (mentioned as modules in the dataset). Different presentations indicated with letters \"B\" and \"J\" after year for semester 2 and semester 1 respectively.\n",
        "\n",
        "    Additionally, the dataset includes student demographics such as location, age group, disability, education level, gender etc.\n",
        "    Student assessment marks, interactions with the Virtual Learning Environment (VLE) are also included.\n",
        "\n",
        "    It contains data about courses, students and their interactions with Virtual Learning Environment (VLE) for seven selected courses (called modules).\n",
        "    Presentations of courses start in February and October - they are marked by â€œBâ€ and â€œJâ€ respectively. The dataset consists of tables connected using unique identifiers.\n",
        "    \"\"\")\n",
        "\n",
        "    st.title(\"Overview of the Datasets:\")\n",
        "\n",
        "    datasets = {\n",
        "        \"VLE\": vle,\n",
        "        \"Student Interactions\": vle_stu,\n",
        "        \"Student Registration\": reg_stu,\n",
        "        \"Student Info\": info_stu,\n",
        "        \"Student Assessments\": as_stu,\n",
        "        \"Courses\": course,\n",
        "        \"Assessments\": assesment\n",
        "    }\n",
        "\n",
        "    dataset_choice = st.selectbox(\"Select Dataset for Overview:\", list(datasets.keys()))\n",
        "\n",
        "    if dataset_choice:\n",
        "        st.subheader(f\"Overview for {dataset_choice}\")\n",
        "        st.write(datasets[dataset_choice][:1000])\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "elif choice == \"Missing Value Analysis\":\n",
        "    st.header(\"Missing Value Treatment\")\n",
        "\n",
        "    datasets = {\n",
        "        \"VLE\": vle,\n",
        "        \"Student Interactions\": vle_stu,\n",
        "        \"Student Registration\": reg_stu,\n",
        "        \"Student Info\": info_stu,\n",
        "        \"Student Assessments\": as_stu,\n",
        "        \"Courses\": course,\n",
        "        \"Assessments\": assesment\n",
        "    }\n",
        "\n",
        "    dataset_choice = st.selectbox(\"Select Dataset for Missing Value Analysis:\", list(datasets.keys()))\n",
        "\n",
        "    if dataset_choice == 'VLE':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Since the 'Week From' and 'Week to' has around 82% missing values, it is best to drop these columns.\")\n",
        "      st.write (\"vle.drop(columns=['week_from','week_to'],inplace=True)\")\n",
        "\n",
        "    elif dataset_choice == 'Student Interactions':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.write('No Missing Values')\n",
        "      st.subheader('Transforming date to datetime')\n",
        "      st.write(\"\"\"\n",
        "      - Define the start date\n",
        "      start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "      - Convert 'date' to datetime\n",
        "      vle_stu['date'] = vle_stu['date'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "      \"\"\")\n",
        "\n",
        "    elif dataset_choice == 'Student Registration':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Since the 'date_unregistration' has around 65% missing values, it is best to drop these columns.\")\n",
        "\n",
        "      st.subheader('Transforming date_registration to datetime')\n",
        "      st.write(\"\"\"\n",
        "      Define the start date\n",
        "      start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "      Convert 'date_submitted' to datetime\n",
        "      reg_stu['date_registration'] = reg_stu['date_registration'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "      \"\"\")\n",
        "\n",
        "    elif dataset_choice == 'Student Info':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Since 'imd_band' has missing value it's better to impute it with dummy value\")\n",
        "      st.write(\"info_stu['imd_band'].fillna('unknown', inplace=True)  # Filling missing values with 'unknown'\")\n",
        "\n",
        "    elif dataset_choice == 'Student Assessments':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.subheader(f\"Missing Value Treatment for {dataset_choice}\")\n",
        "      st.write(\"Score has missing values. Imputing the missing value with the mean of the score achieved that particular student\")\n",
        "      st.write(\"\"\"\n",
        "      student_means = as_stu.groupby('id_student')['score'].transform('mean')\n",
        "      as_stu['score'].fillna(student_means, inplace=True)\n",
        "      \"\"\")\n",
        "\n",
        "      st.subheader('Transforming date_submitted to datetime')\n",
        "      st.write(\"\"\"\n",
        "      Define the start date\n",
        "      start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "      Convert 'date_submitted' to datetime\n",
        "      as_stu['date_submitted'] = as_stu['date_submitted'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "      \"\"\")\n",
        "\n",
        "    elif dataset_choice == 'Courses':\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.write('No Missing Values')\n",
        "\n",
        "    elif dataset_choice == 'Assessments':\n",
        "\n",
        "      st.subheader(f\"Missing Value Analysis for {dataset_choice}\")\n",
        "      missingValueAssessment(datasets[dataset_choice])\n",
        "\n",
        "      st.write(\"'Date' has around 5% missing values so it's better that we drop these missing values as we don't if this exam has been conducted or not\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "elif choice == \"Exploratory Data Analysis (EDA)\":\n",
        "\n",
        "  start_date = pd.Timestamp('2000-01-01')\n",
        "\n",
        "  datasets = [\n",
        "        \"VLE\",\n",
        "        \"Student Interactions\",\n",
        "        \"Student Registration\",\n",
        "        \"Student Info\",\n",
        "        \"Student Assessments\",\n",
        "        \"Courses\",\n",
        "        \"Assessments\"\n",
        "  ]\n",
        "\n",
        "  dataset_choice = st.selectbox(\"Select Dataset for EDA:\", datasets)\n",
        "\n",
        "  if dataset_choice == 'VLE':\n",
        "    st.subheader(f\"EDA for {dataset_choice}\")\n",
        "\n",
        "    vle_cleaned = vle.copy()\n",
        "    vle_cleaned.drop(columns=['week_from','week_to'],inplace=True)\n",
        "\n",
        "    plotActivityCounts(vle_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Interactions':\n",
        "\n",
        "    vle_stu_cleaned = vle_stu.copy()\n",
        "    # Convert 'date' to datetime\n",
        "    vle_stu_cleaned['date'] = vle_stu_cleaned['date'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "\n",
        "\n",
        "    student_vle_eda(vle_stu_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Registration':\n",
        "\n",
        "    reg_stu_cleaned = reg_stu.copy()\n",
        "    reg_stu_cleaned.drop(columns=['date_unregistration'],inplace=True)\n",
        "    reg_stu_cleaned['date_registration'] = pd.to_numeric(reg_stu_cleaned['date_registration'])\n",
        "    reg_stu_cleaned = reg_stu_cleaned.dropna()\n",
        "    reg_stu_cleaned['date_registration'] = reg_stu_cleaned['date_registration'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "\n",
        "    stu_reg_eda(reg_stu_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Info':\n",
        "    info_stu_cleaned = info_stu.copy()\n",
        "    info_stu_cleaned['imd_band'] = info_stu_cleaned['imd_band'].fillna('unknown', inplace=True)\n",
        "    info_stu_cleaned['num_of_prev_attempts'] = info_stu_cleaned['num_of_prev_attempts'].fillna(info_stu['num_of_prev_attempts'].mean(), inplace=True)\n",
        "\n",
        "    info_stu_eda(info_stu_cleaned)\n",
        "\n",
        "  elif dataset_choice == 'Student Assessments':\n",
        "\n",
        "    as_stu_cleaned = as_stu.copy()\n",
        "    student_means = as_stu_cleaned.groupby('id_student')['score'].transform('mean')\n",
        "    as_stu_cleaned['score'] = as_stu_cleaned['score'].fillna(student_means, inplace=True)\n",
        "    as_stu_cleaned['date_submitted'] = as_stu_cleaned['date_submitted'].apply(lambda x: start_date + pd.Timedelta(days=x-1))\n",
        "\n",
        "    as_stu_eda(as_stu_cleaned)\n",
        "\n",
        "elif choice == \"Machine Learning (ML)\":\n",
        "\n",
        "  student_info_assessments = data_merge(as_stu, info_stu)\n",
        "\n",
        "\n",
        "  # Combine the assessments information\n",
        "  student_info_assessments = data_merge(as_stu, info_stu)\n",
        "\n",
        "  models = ['Introduction','Linear Regression', 'Decision Tree Regressor', 'XGBoost', 'ARIMA']\n",
        "\n",
        "  model_choice = st.selectbox(\"Select Model for Prediction:\", models)\n",
        "\n",
        "\n",
        "\n",
        "  if model_choice == 'Introduction':\n",
        "    st.subheader(\"Data Merging and Splitting for Modeling\")\n",
        "\n",
        "    st.write('Data Selected: Student Assessment & Student Information')\n",
        "    st.write('data_merge(as_stu, info_stu)')\n",
        "    st.write(\"Final Dataset Size:: 207319 rows Ã— 15 columns\")\n",
        "    st.write (\"Target Column: score\")\n",
        "    st.write(\"Date column:: date_submitted\")\n",
        "\n",
        "    st.write(student_info_assessments.head(150))\n",
        "\n",
        "    st.write(\"Spliting the data into 80% training set and 20% test set\")\n",
        "    st.write(\"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\")\n",
        "\n",
        "  elif(model_choice == 'Linear Regression'):\n",
        "    linear_reg(student_info_assessments)\n",
        "  elif model_choice == 'Decision Tree Regressor':\n",
        "    decision_tree(student_info_assessments)\n",
        "  elif model_choice == 'XGBoost':\n",
        "    xgboost_mdl(student_info_assessments)\n",
        "  elif model_choice == 'ARIMA':\n",
        "    arima(student_info_assessments)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsroig6WTK3f",
        "outputId": "c8bc2b6f-e138-498c-c79d-3b6c8d6670d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.822s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSDE7B7pTKvt",
        "outputId": "fdd6064c-f292-4b77-91ad-274ed0b8230c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.106.206.146\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/app.py --server.port 8055 &>/content/logs.txt & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttqvmHf9vl7i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDsGG_j8TP4t",
        "outputId": "525679d8-fde3-4a54-e898-32b882d919c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.539s\n",
            "your url is: https://quick-lions-run.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8055\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}